#!/usr/bin/env python
"""Data Analysis & Preparation Script (for scrna-nn)

Usage:
    scrna-nn-data-prep <expression_h5> <ontology_mappings_json> [--unlabeled --filter --term_distances --assign=<assn>]
    scrna-nn-data-prep (-h | --help)
    scrna-nn-data-prep --version

Options:
    --unlabeled         Take all of the data and do not filter any of the samples out. Do not attempt to assign lables.
                        This is useful for unsupervised learning
    --filter            Remove terms that have less than 75 cells mapping to them. Also remove terms that are not part
                        of the main Cell Ontology DAG (i.e. NCBITaxon terms). Also remove some other, pre-determined
                        specific nodes (see the code for this list).
    --term_distances    Print the Jaccard distances between pairs of terms. The Jaccard distance is calculated between
                        the two sets of cells that map to the two terms.
    --assign=<assn>     Assign labels for the cells and split into a query set and a train/database set. The type of
                        assignment is dictated by the value of <assn>, which can be either:
                        - 'all'     Use all ontology terms that a cell maps to. For every term it maps to, create a
                                    copy of the cell's expression, with that term as the label.
                        - 'unique'  Use only cells that map to a single ontology term. Throw out cells that map to
                                    multiple terms.
"""
import math
import json
import sys
# import pdb; pdb.set_trace()
from collections import defaultdict, namedtuple
from typing import List, Tuple
from os import makedirs
from os.path import exists, join

#sys.setrecursionlimit(5000)

import mygene
import numpy as np
import pandas as pd
from docopt import docopt
from scipy.spatial.distance import jaccard


def get_accessions_from_accessionSeries(accessionSeries_list):
    accessions = []
    for accessionSeries in accessionSeries_list:
        accession = accessionSeries.split('_')[0]
        accessions.append(accession)
    return np.array(accessions)


def convert_entrez_to_symbol(entrezIDs):
    mg = mygene.MyGeneInfo()
    result = mg.getgenes(entrezIDs, fields='symbol', species='mouse')
    symbols = [d['symbol'].lower() for d in result]
    return symbols


DataFrameConstructionPOD = namedtuple('DataFrameConstructionPOD', ['true_id_index', 'index', 'expression_vectors', 'labels'])

def assign_all_terms(all_terms: List[str],
                     rpkm_df: pd.DataFrame, mapping_mat: np.ndarray) -> DataFrameConstructionPOD:
    true_id_index = []
    index = []
    expression_vectors = []
    labels = []
    for term_idx in range(mapping_mat.shape[1]):
        term_str = all_terms[term_idx].split()[0]
        if term_str == 'CL:0000000':
            print("skipping 'cell' category")
            continue # This category is way too general, it's never appropriate
        cell_selection_vector = mapping_mat[:, term_idx]
        num_to_add = np.sum(cell_selection_vector)
        old_index_values = rpkm_df.index[cell_selection_vector]
        expression_vectors.extend(rpkm_df.loc[old_index_values].values)
        new_index_values = [cell_id + '_' + term_str for cell_id in old_index_values]
        index.extend(new_index_values)
        true_id_index.extend(old_index_values)
        labels.extend([all_terms[term_idx]] * num_to_add)
    expression_vectors = np.asarray(expression_vectors)
    pod = DataFrameConstructionPOD(true_id_index, index, expression_vectors, labels)
    return pod


def assign_unique_terms(mappings, rpkm_df):
    pod = DataFrameConstructionPOD([], [], [], [])
    selected_cells = []
    selected_labels = []
    for key, value in mappings.items():
        if len(value) == 1:
            selected_cells.append(key)
            selected_labels.append(value[0])
    #rpkm_df.drop_duplicates(inplace=True)

    expression_vectors = rpkm_df.loc[selected_cells].values
    index = rpkm_df.loc[selected_cells].index
    true_id_index = index
    labels = selected_labels
    pod = DataFrameConstructionPOD(true_id_index, index, expression_vectors, labels)
    print("\nThe selected labels and their counts (no overlap):")
    unique_labels, counts = np.unique(selected_labels, return_counts=True)
    for l, c in zip(unique_labels, counts):
        print("\t", l, "\t", c)
    print()
    return pod

def calculate_term_distances(mapping_mat: np.ndarray, terms):
    num_terms = mapping_mat.shape[1]
    jaccard_distances = []
    for i in range(num_terms):
        for j in range(i + 1, num_terms):
            dist = jaccard(mapping_mat[:, i], mapping_mat[:, j])
            jaccard_distances.append(((terms[i], terms[j]), dist))
    sorted_distances = sorted(jaccard_distances, key=lambda x: x[1])
    print("\nSorted Distances (increasing Jaccard distances) (first 50)")
    for i in range(50):
        print(sorted_distances[i])    


def build_mapping_mat(cells, concise_mappings: dict) -> Tuple[np.ndarray, List[str]]:
    from itertools import chain
    terms = list(set(chain.from_iterable(concise_mappings.values()))) # All possible ontology terms
    terms = sorted(terms)
    num_cells = len(cells)
    num_terms = len(terms)
    mapping_mat = np.zeros((num_cells, num_terms), dtype=np.bool_)
    for i in range(num_cells):
        for j in range(num_terms):
            if cells[i] in concise_mappings and terms[j] in concise_mappings[cells[i]]:
                mapping_mat[i, j] = 1
    return mapping_mat, terms


def load_cell_to_ontology_mapping(cells, ontology_mapping):
    empty_count = 0
    mappings = {}
    for cell in cells:
        if cell not in ontology_mapping:
            empty_count += 1
            continue
        terms_for_cell = ontology_mapping[cell]
        if len(terms_for_cell) == 0:
            empty_count += 1
        mappings[cell] = terms_for_cell
    print("Num cells with empty mappings: ", empty_count)
    return mappings


def analyze_cell_to_ontology_mapping(mappings):
    num_terms_mapped_to_l = []
    term_counts_d = defaultdict(int)
    for terms_for_cell in mappings.values():
        num_terms_mapped_to_l.append(len(terms_for_cell))
        for term in terms_for_cell:
            term_counts_d[term] += 1
    print("\nBincount of number of mapped terms for each cell:")
    print(np.bincount(num_terms_mapped_to_l))
    print("\nSorted list of terms by number of cells mapping to them (may overlap):")
    sorted_terms = sorted(term_counts_d.items(), key=lambda item: item[1], reverse=True)
    for term in sorted_terms:
        print(term)
    return term_counts_d


def filter_cell_to_ontology_terms(mappings, term_counts_d):
    terms_to_ignore = set()
    for term, count in term_counts_d.items():
        if count < 75 or 'NCBITaxon' in term or 'PR:' in term or 'PATO:' in term or 'GO:' in term or 'CLO:' in term:
            terms_to_ignore.add(term)
    # Terms that just don't seem that useful, or had too much overlap with another term that was more useful
    terms_to_ignore.add('UBERON:0000006 islet of Langerhans')
    terms_to_ignore.add('CL:0000639 basophil cell of pars distalis of adenohypophysis')
    terms_to_ignore.add('CL:0000557 granulocyte monocyte progenitor cell')
    terms_to_ignore.add('UBERON:0001068 skin of back')
    terms_to_ignore.add('CL:0000034 stem cell')
    terms_to_ignore.add('CL:0000048 multi fate stem cell')
    terms_to_ignore.add('UBERON:0000178 blood')
    terms_to_ignore.add('UBERON:0001135 smooth muscle tissue')
    terms_to_ignore.add('UBERON:0001630 muscle organ')
    terms_to_ignore.add('CL:0000000 cell')
    terms_to_ignore.add('CL:0000080 circulating cell')

    # Clean the mappings
    for cell in mappings.keys():
        terms = mappings[cell]
        mappings[cell] = [term for term in terms if term not in terms_to_ignore]
    return mappings

def write_data_to_npy(foldername, df, labels=None, true_cell_ids=None, gene_symbols=None):
    print(df.shape)
    if not gene_symbols:
        gene_symbols = convert_entrez_to_symbol(df.columns)

    if not exists(foldername):
        makedirs(foldername)

    np.save(join(foldername, 'expr.npy'), df.values)
    np.save(join(foldername, 'cell_IDs.npy'), df.index.values)
    np.save(join(foldername, 'gene_IDs.npy'), df.columns.values)
    np.save(join(foldername, 'gene_symbols.npy'), np.array(gene_symbols))
    accessions = get_accessions_from_accessionSeries(df.index)
    np.save(join(foldername, 'studies.npy'), np.array(accesssions))
    if labels is not None:
        np.save(join(foldername, 'labels.npy'), np.array(labels))
    # if true_cell_ids is not None:
    #     new_h5_store['true_ids'] = pd.Series(data=true_cell_ids, index=df.index)

    return gene_symbols

def write_data_to_h5(name, df, labels=None, true_cell_ids=None, gene_symbols=None):
    print(df.shape)
    if not gene_symbols:
        gene_symbols = convert_entrez_to_symbol(df.columns)
    # Add these as Datasets in the hdf5 file
    gene_symbols_series = pd.Series(data=gene_symbols, index=df.columns)
    accessions = get_accessions_from_accessionSeries(df.index)
    accessions_series = pd.Series(data=accessions, index=df.index)


    new_h5_store = pd.HDFStore(name)
    new_h5_store['rpkm'] = df
    new_h5_store['accessions'] = accessions_series
    new_h5_store['gene_symbols'] = gene_symbols_series

    if labels is not None:
        labels_series = pd.Series(data=labels, index=df.index)    
        new_h5_store['labels'] = labels_series


    if true_cell_ids is not None:
        new_h5_store['true_ids'] = pd.Series(data=true_cell_ids, index=df.index)
    new_h5_store.close()

    return gene_symbols


if __name__ == '__main__':
    args = docopt(__doc__, version='data_prep 0.1')

    # Open the hdf5 file that needs to be prepped (supplied as argument to this script)
    h5_store = pd.HDFStore(args['<expression_h5>'])
    print("loaded h5 file")
    rpkm_df = h5_store['rpkm']
    h5_store.close()
    print(rpkm_df.shape)
    if 'GSE61300_GSM1501785' in rpkm_df.index:
        rpkm_df.drop('GSE61300_GSM1501785', inplace=True) # hack because drop_duplicates doesn't work
    if 'GSE61300_GSM1501786' in rpkm_df.index:
        rpkm_df.drop('GSE61300_GSM1501786', inplace=True)
    rpkm_df.fillna(0, inplace=True)
    with open(args['<ontology_mappings_json>'], 'r') as f:
        cell_to_terms = json.load(f)
    mappings = load_cell_to_ontology_mapping(rpkm_df.index, cell_to_terms)

    if args['--unlabeled']:
        #write_data_to_h5('all_data.h5', rpkm_df)
        write_data_to_npy('all_data_unlabeled', rpkm_df)
        sys.exit()

    if args['--filter']:
        # Analyze the mappings
        print("\n\nBEFORE FILTERING")
        term_counts_d = analyze_cell_to_ontology_mapping(mappings)
        # Filter the mappings
        mappings = filter_cell_to_ontology_terms(mappings, term_counts_d)
        print("\n\nAFTER FILTERING")
        analyze_cell_to_ontology_mapping(mappings)
    mapping_mat = None
    if args['--assign'] == 'all' or args['--term_distances']:
        # These options require a mapping matrix
        mapping_mat, terms = build_mapping_mat(rpkm_df.index, mappings)
        #assigned_rpkm_df, true_id_index, labels = assign_terms(rpkm_df, mapping_mat, terms, args['--uniq_lvl'])
    if args['--term_distances']:
        calculate_term_distances(mapping_mat, terms)
    if args['--assign']:
        if args['--assign'] == 'all':
            pod = assign_all_terms(terms, rpkm_df, mapping_mat)
        elif args['--assign'] == 'unique':
            pod = assign_unique_terms(mappings, rpkm_df)
        assigned_rpkm_df = pd.DataFrame(data=pod.expression_vectors, columns=rpkm_df.columns, index=pod.index)
        accessions = get_accessions_from_accessionSeries(assigned_rpkm_df.index)
        # Find cell types that exist in more than one Accession
        # Of these, pick the accession with the median number of cells of that cell type,
        # hold it out for the query set
        #query_accn_for_label = {}
        valid_accns_for_label = {}
        test_accns_for_label = {}

        label_to_accessions_d = defaultdict(lambda: defaultdict(int))
        for accession, label in zip(accessions, pod.labels):
            label_to_accessions_d[label][accession] += 1

        # For Differential Expression, for a node, we only consider cells from the same study, and require
        # the study to have at least 100 cells (currently). Keep track of how many nodes will satisfy this.
        de_nodes = set()
        
        print("\nAccessions for each cell type:")
        print("* = in validation set")
        print("^ = in test set")
        for label, accession_counts_d in label_to_accessions_d.items():
            print(label)
            accns_for_label = []
            accns_for_label_counts = []
            print("\t<acsn>: <count>")

            for accession, count in accession_counts_d.items():
                accns_for_label.append(accession)
                accns_for_label_counts.append(count)
                if count >= 75:
                    de_nodes.add(label)

            sorted_indices = np.argsort(accns_for_label_counts)
            valid_accns = []
            test_accns = []
            n_accessions = len(accession_counts_d.keys())
            train_cutoff = -1
            valid_test_cutoff = -1
            if n_accessions >= 3:
                train_cutoff = math.floor(0.9 * n_accessions)
                valid_test_cutoff = math.floor(0.5 * train_cutoff)

            accns_for_label = np.array(accns_for_label)
            accns_for_label_counts = np.array(accns_for_label_counts)
            for idx, (accession, count) in enumerate(zip(accns_for_label[sorted_indices], accns_for_label_counts[sorted_indices])):
                if idx < valid_test_cutoff:
                    # will go in test set
                    print("\t^{}: {}".format(accession, count))
                    test_accns.append(accession)
                elif idx < train_cutoff:
                    # will go in valid set
                    print("\t*{}: {}".format(accession, count))
                    valid_accns.append(accession)
                else:
                    # will go in train set
                    print("\t {}: {}".format(accession, count))
            valid_accns_for_label[label] = valid_accns
            test_accns_for_label[label] = test_accns
            
            #     # Find accession with median number of cells of this type:
            #     median_idx = sorted_indices[len(accns_for_label_counts) // 2]
            #     one_before_median_idx = sorted_indices[(len(accns_for_label_counts) // 2)-1]
            #     valid_accns = [ accns_for_label[median_idx] ]
            #     test_accns = [ accns_for_label[one_before_median_idx] ]
            #     if len(accession_counts_d.keys()) >= 5:
            #         one_after_median_idx = sorted_indices[(len(accns_for_label_counts) // 2)+1]
            #         two_before_median_idx = sorted_indices[(len(accns_for_label_counts) // 2)-2]
            #         valid_accns.append(accns_for_label[one_after_median_idx])
            #         test_accns.append(accns_for_label[two_before_median_idx])
            #     valid_accns_for_label[label] = valid_accns
            #     test_accns_for_label[label] = test_accns
            # accns_for_label = np.array(accns_for_label)
            # accns_for_label_counts = np.array(accns_for_label_counts)
            # for accession, count in zip(accns_for_label[sorted_indices], accns_for_label_counts[sorted_indices]):
            #     if accession in valid_accns:
            #         print("\t*{}: {}".format(accession, count))
            #     elif accession in test_accns:
            #         print("\t^{}: {}".format(accession, count))
            #     else:
            #         print("\t {}: {}".format(accession, count))

        # Print out the query cell types
        print("valid/test cell types: ")
        valid_test_types_count = 0
        for cell_type, accn_list in valid_accns_for_label.items():
            if len(accn_list > 0):
                print("\t", cell_type)
                valid_test_types_count += 1
        print("Total valid/test cell types: {}".format(valid_test_types_count))
                

        # Print out nodes that we can do DE analysis for
        print("Num nodes for DE: ", len(de_nodes))
        for n in de_nodes:
            print("\t", n)
        
        # Split the dataset
        traindb_ids = []
        traindb_true_ids = []
        traindb_labels = []

        valid_ids = []
        valid_true_ids = []
        valid_labels = []
        
        test_ids = []
        test_true_ids = []
        test_labels = []
        for cell_id, true_id, label, accession in zip(assigned_rpkm_df.index, pod.true_id_index, pod.labels, accessions):
            if label in valid_accns_for_label and accession in valid_accns_for_label[label]:
                valid_ids.append(cell_id)
                valid_true_ids.append(true_id)
                valid_labels.append(label)
            elif label in test_accns_for_label and accession in test_accns_for_label[label]:
                test_ids.append(cell_id)
                test_true_ids.append(true_id)
                test_labels.append(label)
            else:
                traindb_ids.append(cell_id)
                traindb_true_ids.append(true_id)
                traindb_labels.append(label)

        # gene_symbols = write_data_to_h5('selected_data.h5', assigned_rpkm_df, pod.labels, pod.true_id_index)
        # write_data_to_h5('train_data.h5', assigned_rpkm_df.loc[traindb_ids], traindb_labels, traindb_true_ids,
        #                  gene_symbols)
        # write_data_to_h5('valid_data.h5', assigned_rpkm_df.loc[valid_ids], valid_labels, valid_true_ids,
        #                  gene_symbols)
        # write_data_to_h5('test_data.h5', assigned_rpkm_df.loc[test_ids], test_labels, test_true_ids,
        #                  gene_symbols)
        if not exists('three_splits'):
            makedirs('three_splits')
        # Saving this is redundant and is now costly given the size of our datasets
        # gene_symbols = write_data_to_npy('selected_data', assigned_rpkm_df, pod.labels, pod.true_id_index)
        gene_symbols = write_data_to_npy(join('three_splits','train_data'), assigned_rpkm_df.loc[traindb_ids], traindb_labels, traindb_true_ids,
                         gene_symbols)
        write_data_to_npy(join('three_splits','valid_data'), assigned_rpkm_df.loc[valid_ids], valid_labels, valid_true_ids,
                         gene_symbols)
        write_data_to_npy(join('three_splits','test_data'), assigned_rpkm_df.loc[test_ids], test_labels, test_true_ids,
                         gene_symbols)
        print("Saved new npy files")
    print("done")

