- experiments/2017_10_03-10:30:45
  Comparing: regular siamese vs "flexible" siamese vs "even flexible" siamese
  Models trained on 2017-09-20 and 2017-09-29
  - "flexible" siamese: models trained with the new, flexible Contrastive Loss function
  - "even flexible" siamese: models trained with the same loss function as "flexible" siamese, but the data pair generation
    process evenly distributes the "different" pairs among the different distances. (expected to do better).
  - Need to also try a new metric in the retrieval that is not restricted to calculating scores only at recall positions where
    there is a perfect match.
- experiments/2017_10_03-15:39:07
  Same as the experiment above, but with a new retrieval metric which computes the score at every recall position, instead of only
  at exact matches.
- experiments/2017_10_04-10:30:25
  Comparing number of epochs trained for even_flex_siam_GO_lvls_4_3_2.31
     Last epochs of training for 100:
     	  Epoch 73/100
	  27579/27579 [==============================] - 88s - loss: 0.0046 - val_loss: 0.0052
	  Epoch 74/100
	  27579/27579 [==============================] - 88s - loss: 0.0046 - val_loss: 0.0051
	  Epoch 75/100
	  27579/27579 [==============================] - 88s - loss: 0.0045 - val_loss: 0.0051
	  Epoch 76/100
	  27579/27579 [==============================] - 88s - loss: 0.0045 - val_loss: 0.0050
	  Epoch 77/100
	  27579/27579 [==============================] - 88s - loss: 0.0044 - val_loss: 0.0050
	  Epoch 78/100
	  27579/27579 [==============================] - 88s - loss: 0.0044 - val_loss: 0.0049
	  Epoch 79/100
	  27579/27579 [==============================] - 88s - loss: 0.0043 - val_loss: 0.0049
	  Epoch 80/100
	  27579/27579 [==============================] - 88s - loss: 0.0043 - val_loss: 0.0048
	  Epoch 81/100
	  27579/27579 [==============================] - 88s - loss: 0.0043 - val_loss: 0.0048
	  Epoch 82/100
	  27579/27579 [==============================] - 88s - loss: 0.0042 - val_loss: 0.0047
	  Epoch 83/100
	  27579/27579 [==============================] - 88s - loss: 0.0042 - val_loss: 0.0047
	  Epoch 84/100
	  27579/27579 [==============================] - 88s - loss: 0.0041 - val_loss: 0.0047
	  Epoch 85/100
	  27579/27579 [==============================] - 88s - loss: 0.0041 - val_loss: 0.0046
	  Epoch 86/100
	  27579/27579 [==============================] - 88s - loss: 0.0041 - val_loss: 0.0046
	  Epoch 87/100
	  27579/27579 [==============================] - 88s - loss: 0.0040 - val_loss: 0.0046
	  Epoch 88/100
	  27579/27579 [==============================] - 88s - loss: 0.0040 - val_loss: 0.0045
	  Epoch 89/100
	  27579/27579 [==============================] - 88s - loss: 0.0039 - val_loss: 0.0045
	  Epoch 90/100
	  27579/27579 [==============================] - 88s - loss: 0.0039 - val_loss: 0.0044
	  Epoch 91/100
	  27579/27579 [==============================] - 88s - loss: 0.0039 - val_loss: 0.0044
	  Epoch 92/100
	  27579/27579 [==============================] - 88s - loss: 0.0038 - val_loss: 0.0044
	  Epoch 93/100
	  27579/27579 [==============================] - 88s - loss: 0.0038 - val_loss: 0.0043
	  Epoch 94/100
	  27579/27579 [==============================] - 88s - loss: 0.0038 - val_loss: 0.0043
	  Epoch 95/100
	  27579/27579 [==============================] - 88s - loss: 0.0037 - val_loss: 0.0043
	  Epoch 96/100
	  27579/27579 [==============================] - 88s - loss: 0.0037 - val_loss: 0.0043
	  Epoch 97/100
	  27579/27579 [==============================] - 88s - loss: 0.0037 - val_loss: 0.0042
	  Epoch 98/100
	  27579/27579 [==============================] - 88s - loss: 0.0037 - val_loss: 0.0042
	  Epoch 99/100
	  27579/27579 [==============================] - 88s - loss: 0.0036 - val_loss: 0.0042
	  Epoch 100/100
	  27579/27579 [==============================] - 88s - loss: 0.0036 - val_loss: 0.0041
	  saving model to folder: mouse_genes_2017_09_29_even_flex_siam_GO_lvls_4_3_2.31

     Last epochs of training for 200:
     	  Epoch 170/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 171/200
	  27579/27579 [==============================] - 88s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 172/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 173/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 174/200
	  27579/27579 [==============================] - 90s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 175/200
	  27579/27579 [==============================] - 91s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 176/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 177/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 178/200
	  27579/27579 [==============================] - 89s - loss: 0.0024 - val_loss: 0.0023
	  Epoch 179/200
	  27579/27579 [==============================] - 89s - loss: 0.0023 - val_loss: 0.0023
	  Epoch 180/200
	  27579/27579 [==============================] - 89s - loss: 0.0023 - val_loss: 0.0023
	  Epoch 181/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 182/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 183/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 184/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 185/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 186/200
	  27579/27579 [==============================] - 88s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 187/200
	  27579/27579 [==============================] - 89s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 188/200
	  27579/27579 [==============================] - 89s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 189/200
	  27579/27579 [==============================] - 89s - loss: 0.0023 - val_loss: 0.0022
	  Epoch 190/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0022
	  Epoch 191/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0022
	  Epoch 192/200
	  27579/27579 [==============================] - 89s - loss: 0.0022 - val_loss: 0.0022
	  Epoch 193/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 194/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 195/200
	  27579/27579 [==============================] - 89s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 196/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 197/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 198/200
	  27579/27579 [==============================] - 88s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 199/200
	  27579/27579 [==============================] - 89s - loss: 0.0022 - val_loss: 0.0021
	  Epoch 200/200
	  27579/27579 [==============================] - 91s - loss: 0.0022 - val_loss: 0.0021
	  saving model to folder: mouse_genes_2017_10_03_even_flex_siam_GO_lvls_4_3_2.31_epochs200

     Last epochs of training for 300:
     	  Epoch 272/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0018
	  Epoch 273/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0018
	  Epoch 274/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0018
	  Epoch 275/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0018
	  Epoch 276/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 277/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 278/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 279/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 280/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 281/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 282/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 283/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 284/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 285/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 286/300
	  27579/27579 [==============================] - 87s - loss: 0.0017 - val_loss: 0.0017
	  Epoch 287/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 288/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 289/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 290/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 291/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 292/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 293/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 294/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 295/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 296/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 297/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 298/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 299/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  Epoch 300/300
	  27579/27579 [==============================] - 87s - loss: 0.0016 - val_loss: 0.0017
	  saving model to folder: mouse_genes_2017_10_03_even_flex_siam_GO_lvls_4_3_2.31_epochs300
